# -*- coding: utf-8 -*-
"""NLTK Course - Text Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UN_VCqs_GeoPGBRqxU3OfbUGDP1JwXvj
"""

from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)

! pip install wikipedia
import wikipedia

wikipage = wikipedia.page("Artificial_intelligence").content
print(wikipage)

import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

wiki_sent = []
wiki_sent = sent_tokenize(wikipage)
print(wiki_sent)

from nltk.tokenize import word_tokenize

wiki_words = []
for sent in wiki_sent:
  wiki_words.extend(word_tokenize(sent))
print(wiki_words)

from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords = stopwords.words('english')

word_frequencies = {}
for word in wiki_words:
    if word not in stopwords and word.isalpha():
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
print(word_frequencies)

maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
print(word_frequencies)

sentence_scores = {}
for sent in wiki_sent:
    for word in wiki_words:
        if word.lower() in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]
print(sentence_scores)

import heapq
summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)

summary = ' '.join(summary_sentences)
print(summary)