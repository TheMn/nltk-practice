# -*- coding: utf-8 -*-
"""NLTK Course - Ngram Tagger.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_lmxBS3_mEPyLbQVPvz5V3AbJ3arGpNb
"""

from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)

import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.corpus import brown
from nltk.tag import UnigramTagger
from nltk.tag import DefaultTagger
from nltk.tag import BigramTagger
from nltk.tag import TrigramTagger

nltk.download('brown')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

brown_tagged_sents = brown.tagged_sents(categories='news')
print(brown_tagged_sents)

# we are dividing the data into a test and train to evaluate our taggers.
train_data = brown_tagged_sents[:int(len(brown_tagged_sents) * 0.9)] # [start : end]
test_data = brown_tagged_sents[int(len(brown_tagged_sents) * 0.9):]

unigram_tagger = UnigramTagger(train_data)
print (unigram_tagger.evaluate(test_data))

default_tagger = DefaultTagger('NN')

unigram_tagger = UnigramTagger(train_data, backoff = default_tagger)
print (unigram_tagger.evaluate(test_data))

bigram_tagger = BigramTagger(train_data, backoff = unigram_tagger)
print (bigram_tagger.evaluate(test_data))

trigram_tagger = TrigramTagger(train_data, backoff = bigram_tagger)
print (trigram_tagger.evaluate(test_data))

my_string = "Many text corpora contain linguistic annotations, representing POS tags, named entities, syntactic structures, semantic roles, and so forth. NLTK provides convenient ways to access several of these corpora, and has data packages containing corpora and corpus samples, freely downloadable for use in teaching and research."

my_sent = nltk.sent_tokenize(my_string) # tokenize sentences
mytoken = [] # create empty list
for sent in my_sent: # for each sentence, tokenize words
  mytoken.extend(word_tokenize(sent))
print(mytoken)
print('\n')
print(bigram_tagger.tag(mytoken)) # tag each word using unigram_tagger model